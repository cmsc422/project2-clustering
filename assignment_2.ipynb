{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC 422 Project 2: Unsupervised Learning\n",
    "\n",
    "In this project, we will explore dimensionality reduction (PCA) and\n",
    "clustering.\n",
    "\n",
    "Files you'll edit:\n",
    "\n",
    "    dr.py           Implementation of PCA\n",
    "    clustering.py   Implementation of K-means (and variants)\n",
    "    cifar10.py      Utilities to work with CIFAR10 data\n",
    "\n",
    "Files you might want to look at:\n",
    "\n",
    "    datasets.py     Some simple toy data sets\n",
    "    digits          Digits data\n",
    "    util.py         Utility functions, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datasets\n",
    "import util\n",
    "import dr_soln as dr\n",
    "import cifar10_soln as cifar10\n",
    "import clustering_soln as clustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: PCA (40%)\n",
    "\n",
    "Our first tasks are to implement PCA.  If implemented correctly, these\n",
    "should be 5-line functions (plus the supporting code I've provided):\n",
    "just be sure to use numpy's eigenvalue computation code.  **Implement\n",
    "PCA in the function `pca` in `dr.py`.**\n",
    "\n",
    "Our first test of PCA will be on Gaussian data with a known covariance\n",
    "matrix.  First, let's generate some data and see how it looks, and see\n",
    "what the *sample covariance* is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(422)\n",
    "\n",
    "# Specify the sample covariance of the multivariate normal data\n",
    "Si = util.sqrtm(np.array([[3,2],[2,4]]))\n",
    "x = np.dot(np.random.randn(1000,2), Si)\n",
    "\n",
    "plt.plot(x[:,0], x[:,1], 'b.')\n",
    "np.dot(x.T,x) / np.real(x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: The reason we have to do a matrix square-root on the covariance\n",
    "is because Gaussians are transformed by standard deviations, not by\n",
    "covariances.)\n",
    "\n",
    "Note that the sample covariance of the data is almost exactly the true\n",
    "covariance of the data.  If you run this with 100,000 data points\n",
    "(instead of 1000), you should get something even closer to\n",
    "`[[3,2],[2,4]]`.\n",
    "\n",
    "Now, let's run PCA on this data.  We basically know what should\n",
    "happen, but let's make sure it happens anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P,Z,evals) = dr.pca(x, 2)\n",
    "print('Z:', Z)\n",
    "print('evals:', evals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the largest eigenvalue corresponds to the\n",
    "direction `[ 0.57390327  0.8189231 ]` and the second largest corresponds to\n",
    "the direction `[-0.8189231   0.57390327]`.  We can project the data onto\n",
    "the first eigenvalue and plot it in red, and the second eigenvalue in\n",
    "green.  (Unfortunately we have to do some ugly reshaping to get\n",
    "dimensions to match up.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.dot(np.dot(x, Z[:,0]).reshape(1000,1), Z[:,0].reshape(1,2))\n",
    "x1 = np.dot(np.dot(x, Z[:,1]).reshape(1000,1), Z[:,1].reshape(1,2))\n",
    "plt.plot(x[:,0], x[:,1], 'b.', x0[:,0], x0[:,1], 'r.', x1[:,0], x1[:,1], 'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on a \"real\" example - CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some \"real\" data, we have preprocessed some data from [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) for you. We have extracted 2000 16x16 images from the CIFAR10 dataset and have supplied the dataset to you in the form of a [pickled](https://docs.python.org/3.1/library/pickle.html) python dictionary whose keys correspond to the following:\n",
    "    - 'X' : List with shape (2000, 784) corresponding to the 2000 CIFAR10 images\n",
    "    - 'y' : The labels corresponding to images in X. The labels 0-9 corrspond to the categories, \n",
    "        `[airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, **implement `cifar10.load_data(...)`, so we can easily get load and use pickled data.** We can then take a look at some examples of the data. Fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = cifar10.load_data('data/cifar10.pkl')\n",
    "cifar10.show_imgs(X[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply PCA to our data and see what we find..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P,Z,evals) = dr.pca(X, X.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** **Plot** the explained variance when you include up to $n$ eigenvectors for each possible value of $n$. How many eigenvectors do you have to include before you've accounted for 90% of the variance?  95%? (Hint: see function\n",
    "`cumsum`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Produce your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1:** (Answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something interesting that we can do is to actually visualize the top eigenvectors. You will have to first implement `cifar10.normalize_img(...)`.\n",
    "\n",
    "Let's plot the top 64 eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z.shape)\n",
    "cifar10.show_imgs(Z.T[:64], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Why do we need to normalize the values in the eigenimages when we want to visualize them? What would happen if we did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Do these eigenimages look like images?  Should they?  Do these look like parts of images? Should they? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look at two particular images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "n_pcs = 8\n",
    "img = X[i]\n",
    "pcs = P[i]\n",
    "pc_order = np.argsort(-np.abs(pcs))[:n_pcs]\n",
    "cifar10.show_img(img)\n",
    "print(\"Principal components, sorted:\", pc_order)\n",
    "cifar10.show_imgs(Z.T[pc_order][:n_pcs], normalize=True)\n",
    "print(\"*\" * 20)\n",
    "i = 55\n",
    "img = X[i]\n",
    "pcs = P[i]\n",
    "pc_order = np.argsort(-np.abs(pcs))[:n_pcs]\n",
    "cifar10.show_img(img)\n",
    "print(\"Principal components, sorted:\", pc_order)\n",
    "cifar10.show_imgs(Z.T[pc_order][:10], labels=pc_order[:], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** Look at the image above, look carefully at the principal components 0, 1, and 2. How do these relate to the above two images? Why do these principal components make sense (hint: say some thing about how most of the images in the dataset are composed/look like)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: K-means clustering (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second task is to implement the largest distance heuristic for\n",
    "kmeans clustering in `clustering.py`.\n",
    "\n",
    "Note that we have already implemented the first initialization heuristic, the *deterministic* heuristic, where the initial K means are simply the first K data points the algorithm recieves. You will have to **implement `clustering.kmeans(...)`**.\n",
    "\n",
    "Then we can quickly run through some basic experiments k-means:\n",
    "\n",
    "**Hint:** While running, this will plot the results.  If you want\n",
    "to turn that off, comment out the obvious line in the `kmeans`\n",
    "function.  Plus, when it says \"Press enter to continue\", if you type\n",
    "\"q\" and press enter, it will stop bugging you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = clustering.initialize_clusters(datasets.X2d, 2, 'determ')\n",
    "(mu,z,obj) = clustering.kmeans(datasets.X2d, mu0)\n",
    "print('mu', mu)\n",
    "print('z', z)\n",
    "print('obj', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also play with another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = clustering.initialize_clusters(datasets.X2d2, 4, 'determ')\n",
    "(mu,z,obj) = clustering.kmeans(datasets.X2d2, mu0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **implement the furthest first heuristic (`ffh`)**, and we can use this heuristic with K-means to cluster the CIFAR10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = clustering.initialize_clusters(X, 15, 'ffh')\n",
    "(mu,z,obj) = clustering.kmeans(X, mu0, doPlot=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** **Make a plot** of the K-means objective against iteration number. Why does the plot have a long tail? Why do we expect the plot to have this shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your plot here\n",
    "### TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can visualize the clusters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plotDatasetClusters(X[:,:2], mu[:, :2], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Why don't we see clear clusters here? what did we do wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do something better then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plotDatasetClusters(P[:,:2], mu[:, :2], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Is this plot better? Why does plotting the images using the first two PCs make sense? Qualitatively, what can we say about the clusters?\n",
    "\n",
    "(Note: If you are having a hard time seeing the colors/clusters, please ask the instructors for more assistance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating K-means (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now implement the `km++` (K Means++), and `random` heuristics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** **Run kmeans 5 times each** with 15 centroids for the kmeans++, furthest first, and random selection, heuristics. For each heuristic, **answer and do** the following.\n",
    "- For each heuristic, plot **one** plot of iteration against objective. (Each plot should have 5 lines).\n",
    "- What was the average number of iterations it took for k-means to converge\n",
    "- Which was the best heuristic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE FOR Q7 HERE. Insert cells for different runs and answers as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A8:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9:** Out of all your runs, select your \"best\" run (the run with the lowest objective) and, for the clustering from the best run, report the following:\n",
    "- For each cluster:\n",
    "    - 1) Report the modal class\n",
    "    - 2) Report the percentage of the cluster that is the modal class\n",
    "    - 3) Display 20 (a 2x10 grid) of sample points from the cluster\n",
    "    - 4) Display the cluster mean\n",
    "    - 5) Describe the cluster using one or two sentences.\n",
    "- **Hint:**, for each fold, your output for 1-4) might look like:\n",
    "<img src=\"kmeans_example_analysis.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE FOR Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A9:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10:**\n",
    "- How successful was the clustering? Write a paragraph (5-7 sentences) describing how \n",
    "- Look at your sample images from your clusters and your cluster centroids, you should see a cluster whose modal label is airplane or boat (look for the cluster mean that vaguely looks like an object in the sky/ocean). Is this a good cluster? Why is it a good cluster? Why are both images of planes and boats in this cluster; how did kmeans get confused? What did kmeans \"learn\" (what features of the image did the algorithm pick up on)? Why might confusing images of boats and planes be problematic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A10:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit! - \"Really\" evaluating K-means..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR10 dataset that we have provided you is nice to play with because we have ground truth labels. However, in the real world, we don't use unsupervised learning on labeled datasets. For extra credit, try the some of the following:\n",
    "\n",
    "- Without CIFAR10 labels, can you try to find out how many clusters or classes of images there really are? Try K-means clustering with different Ks and evaluate your result by implementing a metric like [silhuotte width](https://en.wikipedia.org/wiki/Silhouette_(clustering)). What patterns do you find, are there \"subclasses\" of classes in the dataset? Are there other patterns in the dataset other than just the 10 classes? Are the clusters any good? Can you use other featurizations or dimention reduction algorithms (like PCA) to improve your clustering? Why do they work?\n",
    "\n",
    "- Or, find or generate an unlabeled dataset, perform dimention reduction and clustering and evaluate your findings.\n",
    "\n",
    "Just show us that you have learned something new and attempt to apply the new things you have learned! Have fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
